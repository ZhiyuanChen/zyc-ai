@inproceedings{AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@inproceedings{Attention,
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  booktitle = {International Conference on Learning Representations},
  editor    = {Yoshua Bengio and Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year      = {2015}
}

@inproceedings{BEiT,
  author    = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  booktitle = {International Conference on Learning Representations},
  title     = {{BE}iT: {BERT} Pre-Training of Image Transformers},
  url       = {https://openreview.net/forum?id=p-BhZSz59o4},
  year      = {2022}
}

@inproceedings{BERT,
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  address   = {Minneapolis, Minnesota},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1423},
  month     = {June},
  pages     = {4171-4186},
  publisher = {Association for Computational Linguistics},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url       = {https://aclanthology.org/N19-1423},
  year      = {2019}
}

@inproceedings{CeiT,
  author    = {Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {579-588},
  title     = {Incorporating Convolution Designs Into Visual Transformers},
  year      = {2021}
}

@inproceedings{CvT,
  author    = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {22-31},
  title     = {CvT: Introducing Convolutions to Vision Transformers},
  year      = {2021}
}

@article{data2vec,
  author     = {Alexei Baevski and Wei{-}Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2202-03555.bib},
  eprint     = {2202.03555},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Wed, 09 Feb 2022 15:43:34 +0100},
  title      = {data2vec: {A} General Framework for Self-supervised Learning in Speech, Vision and Language},
  url        = {https://arxiv.org/abs/2202.03555},
  volume     = {abs/2202.03555},
  year       = {2022}
}

@inproceedings{DeiT,
  abstract  = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
  author    = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18-24 Jul},
  pages     = {10347--10357},
  pdf       = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {Training data-efficient image transformers &amp; distillation through attention},
  url       = {https://proceedings.mlr.press/v139/touvron21a.html},
  volume    = {139},
  year      = {2021}
}

@article{ILSVRC,
  author  = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  doi     = {10.1007/s11263-015-0816-y},
  journal = {International Journal of Computer Vision (IJCV)},
  number  = {3},
  pages   = {211-252},
  title   = {ImageNet Large Scale Visual Recognition Challenge},
  volume  = {115},
  year    = {2015}
}

@inproceedings{ImageNet,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  month     = {June},
  pages     = {248-255},
  title     = {Imagenet: A large-scale hierarchical image database},
  year      = {2009}
}

@article{LeNet,
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  author   = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
  doi      = {10.1162/neco.1989.1.4.541},
  eprint   = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
  issn     = {0899-7667},
  journal  = {Neural Computation},
  month    = {12},
  number   = {4},
  pages    = {541-551},
  title    = {Backpropagation Applied to Handwritten Zip Code Recognition},
  url      = {https://doi.org/10.1162/neco.1989.1.4.541},
  volume   = {1},
  year     = {1989}
}

@inproceedings{LeViT,
  author    = {Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J\'egou, Herv\'e and Douze, Matthijs},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {12259-12269},
  title     = {LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference},
  year      = {2021}
}

@inproceedings{MAE,
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  pages     = {16000-16009},
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  year      = {2022}
}

@inproceedings{MaskFeat,
  author    = {Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  pages     = {14668-14678},
  title     = {Masked Feature Prediction for Self-Supervised Visual Pre-Training},
  year      = {2022}
}

@inproceedings{MoCo,
  author    = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  year      = {2020}
}

@inproceedings{ResNet,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016}
}

@inproceedings{SimCLR,
  abstract  = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
  articleno = {149},
  author    = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  numpages  = {11},
  publisher = {JMLR.org},
  series    = {ICML'20},
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  year      = {2020}
}

@inproceedings{Swin,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {10012-10022},
  title     = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
  year      = {2021}
}

@inproceedings{Transformer,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@inproceedings{VGG,
  author    = {Karen Simonyan and Andrew Zisserman},
  booktitle = {International Conference on Learning Representations},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year      = {2015}
}

@inproceedings{Visformer,
  author    = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {589-598},
  title     = {Visformer: The Vision-Friendly Transformer},
  year      = {2021}
}

@inproceedings{ViT,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {International Conference on Learning Representations},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
  year      = {2021}
}