@inproceedings{AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@inproceedings{Annotations1.3B,
  author    = {Beal, Josh and Wu, Hao-Yu and Park, Dong Huk and Zhai, Andrew and Kislyuk, Dmitry},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  month     = {January},
  pages     = {564-573},
  title     = {Billion-Scale Pretraining With Vision Transformers for Multi-Task Visual Representations},
  year      = {2022}
}

@inproceedings{Attention,
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  booktitle = {International Conference on Learning Representations},
  editor    = {Yoshua Bengio and Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year      = {2015}
}

@inproceedings{bachman2019learning,
  author    = {Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Learning Representations by Maximizing Mutual Information Across Views},
  url       = {https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{BEiT,
  author    = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  booktitle = {International Conference on Learning Representations},
  title     = {{BE}iT: {BERT} Pre-Training of Image Transformers},
  url       = {https://openreview.net/forum?id=p-BhZSz59o4},
  year      = {2022}
}

@inproceedings{BERT,
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  address   = {Minneapolis, Minnesota},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1423},
  month     = {June},
  pages     = {4171-4186},
  publisher = {Association for Computational Linguistics},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url       = {https://aclanthology.org/N19-1423},
  year      = {2019}
}

@inproceedings{bot,
  author    = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  title     = {Bag of Tricks for Image Classification with Convolutional Neural Networks},
  year      = {2019}
}

@inproceedings{CeiT,
  author    = {Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {579-588},
  title     = {Incorporating Convolution Designs Into Visual Transformers},
  year      = {2021}
}

@inproceedings{CLIP,
  abstract  = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18-24 July},
  pages     = {8748-8763},
  pdf       = {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  url       = {https://proceedings.mlr.press/v139/radford21a.html},
  volume    = {139},
  year      = {2021}
}

@inproceedings{CMC,
  abstract  = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ``dog'' can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Code is available at: http://github.com/HobbitLong/CMC/.},
  address   = {Cham},
  author    = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  editor    = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  isbn      = {978-3-030-58621-8},
  pages     = {776-794},
  publisher = {Springer International Publishing},
  title     = {Contrastive Multiview Coding},
  year      = {2020}
}

@inproceedings{COCO,
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollar, Piotr and Zitnick, Larry},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  month     = {September},
  pages     = {740-755},
  publisher = {Springer International Publishing},
  title     = {{Microsoft COCO}: Common Objects in Context},
  url       = {https://www.microsoft.com/en-us/research/publication/microsoft-coco-common-objects-in-context/},
  year      = {2014}
}

@article{ConVIRT,
  author     = {Yuhao Zhang and Hang Jiang and Yasuhide Miura and Christopher D. Manning and Curtis P. Langlotz},
  eprint     = {2010.00747},
  eprinttype = {arXiv},
  journal    = {CoRR},
  title      = {Contrastive Learning of Medical Visual Representations from Paired Images and Text},
  url        = {https://arxiv.org/abs/2010.00747},
  volume     = {abs/2010.00747},
  year       = {2020}
}

@inproceedings{CvT,
  author    = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {22-31},
  title     = {{CvT}: Introducing Convolutions to Vision Transformers},
  year      = {2021}
}

@article{data2vec,
  author     = {Alexei Baevski and Wei{-}Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},
  eprint     = {2202.03555},
  eprinttype = {arXiv},
  journal    = {CoRR},
  title      = {{data2vec}: {A} General Framework for Self-supervised Learning in Speech, Vision and Language},
  url        = {https://arxiv.org/abs/2202.03555},
  volume     = {abs/2202.03555},
  year       = {2022}
}

@inproceedings{DeiT,
  abstract  = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
  author    = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18-24 July},
  pages     = {10347-10357},
  pdf       = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {Training data-efficient image transformers &amp; distillation through attention},
  url       = {https://proceedings.mlr.press/v139/touvron21a.html},
  volume    = {139},
  year      = {2021}
}

@inproceedings{EfficientNet,
  abstract  = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  author    = {Tan, Mingxing and Le, Quoc},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month     = {09-15 Jun},
  pages     = {6105-6114},
  pdf       = {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  url       = {https://proceedings.mlr.press/v97/tan19a.html},
  volume    = {97},
  year      = {2019}
}

@inproceedings{ELMo,
  abstract  = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  address   = {New Orleans, Louisiana},
  author    = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  doi       = {10.18653/v1/N18-1202},
  month     = {June},
  pages     = {2227-2237},
  publisher = {Association for Computational Linguistics},
  title     = {Deep Contextualized Word Representations},
  url       = {https://aclanthology.org/N18-1202},
  year      = {2018}
}

@inproceedings{ERNIE,
  abstract  = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.},
  address   = {Florence, Italy},
  author    = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  doi       = {10.18653/v1/P19-1139},
  month     = jul,
  pages     = {1441-1451},
  publisher = {Association for Computational Linguistics},
  title     = {{ERNIE}: Enhanced Language Representation with Informative Entities},
  url       = {https://aclanthology.org/P19-1139},
  year      = {2019}
}

@article{GPT,
  author    = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  publisher = {OpenAI},
  title     = {Improving language understanding by generative pre-training},
  year      = {2018}
}

@article{GPT-2,
  author    = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  publisher = {OpenAI},
  title     = {Language Models are Unsupervised Multitask Learners},
  year      = {2018}
}

@inproceedings{GPT-3,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877-1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@article{hestness2017deep,
  author     = {Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory F. Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
  eprint     = {1712.00409},
  eprinttype = {arXiv},
  journal    = {CoRR},
  title      = {Deep Learning Scaling is Predictable, Empirically},
  url        = {http://arxiv.org/abs/1712.00409},
  volume     = {abs/1712.00409},
  year       = {2017}
}

@inproceedings{IG1B,
  author    = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  month     = {September},
  title     = {Exploring the Limits of Weakly Supervised Pretraining},
  year      = {2018}
}

@article{ILSVRC,
  author  = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  doi     = {10.1007/s11263-015-0816-y},
  journal = {International Journal of Computer Vision (IJCV)},
  number  = {3},
  pages   = {211-252},
  title   = {ImageNet Large Scale Visual Recognition Challenge},
  volume  = {115},
  year    = {2015}
}

@inproceedings{ImageGPT,
  abstract  = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.},
  author    = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13-18 Jul},
  pages     = {1691-1703},
  pdf       = {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {Generative Pretraining From Pixels},
  url       = {https://proceedings.mlr.press/v119/chen20s.html},
  volume    = {119},
  year      = {2020}
}

@inproceedings{ImageNet,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  month     = {June},
  pages     = {248-255},
  title     = {{ImageNet}: A large-scale hierarchical image database},
  year      = {2009}
}

@inproceedings{JFT300M,
  author    = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {October},
  title     = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
  year      = {2017}
}

@inproceedings{JFT3B,
  author    = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  pages     = {12104-12113},
  title     = {Scaling Vision Transformers},
  year      = {2022}
}

@article{LeNet,
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  author   = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
  doi      = {10.1162/neco.1989.1.4.541},
  eprint   = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
  issn     = {0899-7667},
  journal  = {Neural Computation},
  month    = {December},
  number   = {4},
  pages    = {541-551},
  title    = {Backpropagation Applied to Handwritten Zip Code Recognition},
  url      = {https://doi.org/10.1162/neco.1989.1.4.541},
  volume   = {1},
  year     = {1989}
}

@inproceedings{LeViT,
  author    = {Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J\'egou, Herv\'e and Douze, Matthijs},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {12259-12269},
  title     = {{LeViT}: A Vision Transformer in ConvNet's Clothing for Faster Inference},
  year      = {2021}
}

@inproceedings{li2017learning,
  author    = {Li, Ang and Jabri, Allan and Joulin, Armand and van der Maaten, Laurens},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {October},
  title     = {Learning Visual N-Grams From Web Data},
  year      = {2017}
}

@inproceedings{MAE,
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  pages     = {16000-16009},
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  year      = {2022}
}

@inproceedings{MaskFeat,
  author    = {Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  pages     = {14668-14678},
  title     = {Masked Feature Prediction for Self-Supervised Visual Pre-Training},
  year      = {2022}
}

@inproceedings{MCNNSIA,
  abstract  = {Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks leads to performance degradation; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling. The technique is general and can be incorporated across layer types and applications, such as image classification and conditional image generation. In addition to increased shift-invariance, we also observe, surprisingly, that anti-aliasing boosts accuracy in ImageNet classification, across several commonly-used architectures. This indicates that anti-aliasing serves as effective regularization. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks.},
  author    = {Zhang, Richard},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month     = {09-15 June},
  pages     = {7324-7334},
  pdf       = {http://proceedings.mlr.press/v97/zhang19a/zhang19a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {Making Convolutional Networks Shift-Invariant Again},
  url       = {https://proceedings.mlr.press/v97/zhang19a.html},
  volume    = {97},
  year      = {2019}
}

@inproceedings{MoCo,
  author    = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  year      = {2020}
}

@article{MoCov2,
  author     = {Xinlei Chen and Haoqi Fan and Ross B. Girshick and Kaiming He},
  eprint     = {2003.04297},
  eprinttype = {arXiv},
  journal    = {CoRR},
  title      = {Improved Baselines with Momentum Contrastive Learning},
  url        = {https://arxiv.org/abs/2003.04297},
  volume     = {abs/2003.04297},
  year       = {2020}
}

@inproceedings{mori1999image,
  author       = {Mori, Yasuhide and Takahashi, Hironobu and Oka, Ryuichi},
  booktitle    = {First international workshop on multimedia intelligent storage and retrieval management},
  organization = {Citeseer},
  pages        = {1-9},
  title        = {Image-to-word transformation based on dividing and vector quantizing images with words},
  year         = {1999}
}

@inproceedings{NoisyStudent,
  author    = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  title     = {Self-Training With Noisy Student Improves ImageNet Classification},
  year      = {2020}
}

@inproceedings{OFA,
  abstract  = {In this work, we pursue a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision &amp; language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.},
  author    = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  month     = {17-23 Jul},
  pages     = {23318-23340},
  pdf       = {https://proceedings.mlr.press/v162/wang22al/wang22al.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {{OFA}: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  url       = {https://proceedings.mlr.press/v162/wang22al.html},
  volume    = {162},
  year      = {2022}
}

@inproceedings{ResNet,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016}
}

@misc{RoFormer,
  archiveprefix = {arXiv},
  author        = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  eprint        = {2104.09864},
  primaryclass  = {cs.CL},
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  year          = {2021}
}

@article{ScalingLaws,
  author     = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  eprint     = {2001.08361},
  eprinttype = {arXiv},
  journal    = {CoRR},
  title      = {Scaling Laws for Neural Language Models},
  url        = {https://arxiv.org/abs/2001.08361},
  volume     = {abs/2001.08361},
  year       = {2020}
}

@inproceedings{SimCLR,
  abstract  = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  author    = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13-18 July},
  pages     = {1597-1607},
  pdf       = {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  url       = {https://proceedings.mlr.press/v119/chen20j.html},
  volume    = {119},
  year      = {2020}
}

@inproceedings{SimCLRv2,
  author    = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {22243-22255},
  publisher = {Curran Associates, Inc.},
  title     = {Big Self-Supervised Models are Strong Semi-Supervised Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{Subword,
  address   = {Berlin, Germany},
  author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi       = {10.18653/v1/P16-1162},
  month     = {August},
  pages     = {1715-1725},
  publisher = {Association for Computational Linguistics},
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  url       = {https://aclanthology.org/P16-1162},
  year      = {2016}
}

@inproceedings{Swin,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {10012-10022},
  title     = {{Swin Transformer}: Hierarchical Vision Transformer Using Shifted Windows},
  year      = {2021}
}

@article{T5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal = {Journal of Machine Learning Research},
  number  = {140},
  pages   = {1-67},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  url     = {http://jmlr.org/papers/v21/20-074.html},
  volume  = {21},
  year    = {2020}
}

@inproceedings{TimeSformer,
  abstract  = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named “TimeSformer,” adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that “divided attention,” where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.},
  author    = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18-24 July},
  pages     = {813-824},
  pdf       = {http://proceedings.mlr.press/v139/bertasius21a/bertasius21a.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {Is Space-Time Attention All You Need for Video Understanding?},
  url       = {https://proceedings.mlr.press/v139/bertasius21a.html},
  volume    = {139},
  year      = {2021}
}

@inproceedings{Transformer,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@inproceedings{ULMFiT,
  abstract  = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.},
  address   = {Melbourne, Australia},
  author    = {Howard, Jeremy and Ruder, Sebastian},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi       = {10.18653/v1/P18-1031},
  month     = {July},
  pages     = {328-339},
  publisher = {Association for Computational Linguistics},
  title     = {Universal Language Model Fine-tuning for Text Classification},
  url       = {https://aclanthology.org/P18-1031},
  year      = {2018}
}

@inproceedings{UniPerceiver,
  author    = {Zhu, Xizhou and Zhu, Jinguo and Li, Hao and Wu, Xiaoshi and Li, Hongsheng and Wang, Xiaohua and Dai, Jifeng},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  pages     = {16804-16815},
  title     = {{Uni-Perceiver}: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks},
  year      = {2022}
}

@article{UniPerceiverMoE,
  author     = {Jinguo Zhu and Xizhou Zhu and Wenhai Wang and Xiaohua Wang and Hongsheng Li and Xiaogang Wang and Jifeng Dai},
  doi        = {10.48550/arXiv.2206.04674},
  eprint     = {2206.04674},
  eprinttype = {arXiv},
  journal    = {CoRR},
  title      = {{Uni-Perceiver-MoE}: Learning Sparse Generalist Models with Conditional MoEs},
  url        = {https://doi.org/10.48550/arXiv.2206.04674},
  volume     = {abs/2206.04674},
  year       = {2022}
}

@inproceedings{UniT,
  author    = {Hu, Ronghang and Singh, Amanpreet},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {1439-1449},
  title     = {{UniT}: Multimodal Multitask Learning With a Unified Transformer},
  year      = {2021}
}

@inproceedings{VATT,
  author    = {Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {24206-24221},
  publisher = {Curran Associates, Inc.},
  title     = {{VATT}: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text},
  url       = {https://proceedings.neurips.cc/paper/2021/file/cb3213ada48302953cb0f166464ab356-Paper.pdf},
  volume    = {34},
  year      = {2021}
}

@inproceedings{VGG,
  author    = {Karen Simonyan and Andrew Zisserman},
  booktitle = {International Conference on Learning Representations},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year      = {2015}
}

@inproceedings{ViLT,
  abstract  = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
  author    = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18-24 July},
  pages     = {5583-5594},
  pdf       = {http://proceedings.mlr.press/v139/kim21k/kim21k.pdf},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  title     = {{ViLT}: Vision-and-Language Transformer Without Convolution or Region Supervision},
  url       = {https://proceedings.mlr.press/v139/kim21k.html},
  volume    = {139},
  year      = {2021}
}

@inproceedings{VirTex,
  author    = {Desai, Karan and Johnson, Justin},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  pages     = {11162-11173},
  title     = {{VirTex}: Learning Visual Representations From Textual Annotations},
  year      = {2021}
}

@inproceedings{Visformer,
  author    = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  pages     = {589-598},
  title     = {{Visformer}: The Vision-Friendly Transformer},
  year      = {2021}
}

@article{VisualGenome,
  abstract   = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that "the person is riding a horse-drawn carriage." In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of $$35$$35 objects, $$26$$26 attributes, and $$21$$21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.},
  address    = {USA},
  author     = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  doi        = {10.1007/s11263-016-0981-7},
  issn       = {0920-5691},
  issue_date = {May 2017},
  journal    = {International Journal of Computer Vision},
  keywords   = {Relationships, Knowledge, Dataset, Language, Image, Computer vision, Crowdsourcing, Scene graph, Objects, Attributes, Question answering},
  month      = {May},
  number     = {1},
  numpages   = {42},
  pages      = {32–73},
  publisher  = {Kluwer Academic Publishers},
  title      = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  url        = {https://doi.org/10.1007/s11263-016-0981-7},
  volume     = {123},
  year       = {2017}
}

@inproceedings{ViT,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {International Conference on Learning Representations},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
  year      = {2021}
}

@article{VT,
  author     = {Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Masayoshi Tomizuka and Kurt Keutzer and Peter Vajda},
  eprint     = {2006.03677},
  eprinttype = {arXiv},
  journal    = {CoRR},
  title      = {Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
  url        = {https://arxiv.org/abs/2006.03677},
  volume     = {abs/2006.03677},
  year       = {2020}
}

@online{wikidump,
  author = {Wikimedia Foundation},
  title  = {Wikimedia Downloads},
  url    = {https://dumps.wikimedia.org}
}

@article{YFCC100M,
  abstract   = {This publicly available curated dataset of almost 100 million photos and videos is free and legal for all.},
  address    = {New York, NY, USA},
  author     = {Thomee, Bart and Shamma, David A. and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  doi        = {10.1145/2812802},
  issn       = {0001-0782},
  issue_date = {February 2016},
  journal    = {Commun. ACM},
  month      = {jan},
  number     = {2},
  numpages   = {10},
  pages      = {64–73},
  publisher  = {Association for Computing Machinery},
  title      = {{YFCC100M}: The New Data in Multimedia Research},
  url        = {https://doi.org/10.1145/2812802},
  volume     = {59},
  year       = {2016}
}

@inproceedings{zhu2015aligning,
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {December},
  title     = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  year      = {2015}
}