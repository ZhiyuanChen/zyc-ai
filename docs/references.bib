@inproceedings{AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@inproceedings{Attention,
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {International Conference on Learning Representations},
  editor    = {Yoshua Bengio and Yann LeCun},
  year      = {2015}
}

@inproceedings{BEiT,
  author    = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  title     = {{BE}iT: {BERT} Pre-Training of Image Transformers},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{BERT,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {June},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171-4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{CeiT,
  author    = {Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
  title     = {Incorporating Convolution Designs Into Visual Transformers},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {579-588}
}

@inproceedings{CvT,
  author    = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  title     = {CvT: Introducing Convolutions to Vision Transformers},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {22-31}
}

@article{data2vec,
  author     = {Alexei Baevski and Wei{-}Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},
  title      = {data2vec: {A} General Framework for Self-supervised Learning in Speech, Vision and Language},
  journal    = {CoRR},
  volume     = {abs/2202.03555},
  year       = {2022},
  url        = {https://arxiv.org/abs/2202.03555},
  eprinttype = {arXiv},
  eprint     = {2202.03555},
  timestamp  = {Wed, 09 Feb 2022 15:43:34 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2202-03555.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DeiT,
  author    = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  title     = {Training data-efficient image transformers &amp; distillation through attention},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {10347--10357},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  month     = {18-24 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf},
  url       = {https://proceedings.mlr.press/v139/touvron21a.html},
  abstract  = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.}
}

@article{ILSVRC,
  author  = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  title   = {ImageNet Large Scale Visual Recognition Challenge},
  journal = {International Journal of Computer Vision (IJCV)},
  year    = {2015},
  doi     = {10.1007/s11263-015-0816-y},
  volume  = {115},
  number  = {3},
  pages   = {211-252}
}

@inproceedings{ImageNet,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  title     = {Imagenet: A large-scale hierarchical image database},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  pages     = {248-255},
  month     = {June},
  year      = {2009}
}

@article{LeNet,
  author   = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
  title    = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
  journal  = {Neural Computation},
  volume   = {1},
  number   = {4},
  pages    = {541-551},
  year     = {1989},
  month    = {12},
  abstract = {{The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1989.1.4.541},
  url      = {https://doi.org/10.1162/neco.1989.1.4.541},
  eprint   = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf}
}

@inproceedings{LeViT,
  author    = {Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J\'egou, Herv\'e and Douze, Matthijs},
  title     = {LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {12259-12269}
}

@inproceedings{MAE,
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {16000-16009}
}

@inproceedings{MaskFeat,
  author    = {Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  title     = {Masked Feature Prediction for Self-Supervised Visual Pre-Training},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {14668-14678}
}

@inproceedings{ResNet,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}

@inproceedings{Swin,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  title     = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {10012-10022}
}

@inproceedings{Transformer,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  title     = {Attention is All you Need},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@inproceedings{VGG,
  author    = {Karen Simonyan and Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {International Conference on Learning Representations},
  year      = {2015}
}

@inproceedings{Visformer,
  author    = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
  title     = {Visformer: The Vision-Friendly Transformer},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {589-598}
}

@inproceedings{ViT,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YicbFdNTTy}
}