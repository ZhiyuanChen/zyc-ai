@inproceedings{AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@inproceedings{BERT,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {June},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171-4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{CeiT,
  author    = {Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
  title     = {Incorporating Convolution Designs Into Visual Transformers},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {579-588}
}

@inproceedings{CvT,
  author    = {Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  title     = {CvT: Introducing Convolutions to Vision Transformers},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {22-31}
}

@inproceedings{ImageNet,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  title     = {Imagenet: A large-scale hierarchical image database},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  pages     = {248-255},
  month     = {June},
  year      = {2009}
}

@article{ILSVRC,
  author  = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  title   = {ImageNet Large Scale Visual Recognition Challenge},
  year    = {2015},
  journal = {International Journal of Computer Vision (IJCV)},
  doi     = {10.1007/s11263-015-0816-y},
  volume  = {115},
  number  = {3},
  pages   = {211-252}
}

@article{LeNet,
  author   = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
  title    = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
  journal  = {Neural Computation},
  volume   = {1},
  number   = {4},
  pages    = {541-551},
  year     = {1989},
  month    = {12},
  abstract = {{The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}},
  issn     = {0899-7667},
  doi      = {10.1162/neco.1989.1.4.541},
  url      = {https://doi.org/10.1162/neco.1989.1.4.541},
  eprint   = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf}
}

@inproceedings{LeViT,
  author    = {Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J\'egou, Herv\'e and Douze, Matthijs},
  title     = {LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {12259-12269}
}

@inproceedings{ResNet,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}

@inproceedings{Visformer,
  author    = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
  title     = {Visformer: The Vision-Friendly Transformer},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {589-598}
}

@inproceedings{VGG,
  author    = {Karen Simonyan and Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {International Conference on Learning Representations},
  year      = {2015}
}

@inproceedings{ViT,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{Swin,
  author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  title     = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2021},
  pages     = {10012-10022}
}