---
authors:
    - zyc
date: 2021-02-13 13:31:14
categories:
    - sketch
    - career
tags:
    - sketch
    - career
    - interview
---

# ResNet面试简介

!!! abstract "前言"

    作为计算机视觉的基础，ResNet几乎在每处都有应用--无论是分类还是检测，VC还是HOI。

    因此，ResNet是我的必问题。令人遗憾的是，至今没有一个人能够回答出所有点。事实上，我有时被面试时也被问到这个问题。但令人更加遗憾的是，有些面试官也不甚了解……

“请简要介绍ResNet。”

这道问题看似简单--带有一个跳跃连接的神经网络。但他其实是对神经网络的一个综合的问题。具体来说，这包含三个方面：神经网络退化，梯度消失/爆炸，非线性、初始化与归一化。

!!! success "ResNet"

    ResNet通过将多个神经网络的层聚合成一个块，然后在这个块的一侧加入一个恒等映射，使得这个块从原本的$F(x)$变成$F(x) + x$，从而解决了神经网络的退化问题。

如果候选人能够给出类似这样的标准回答，那之后话题就会很自然而然地转移到网络退化与恒等映射去，另外两点只能找机会再问。然而，绝大多数人的回答都是：

## 梯度消失/爆炸

!!! failure "梯度消失/爆炸"

    ResNet通过引入跳跃连接，使得梯度能够更好的回传，从而缓解了梯度消失与梯度爆炸问题。

虽然这是一个不太正确的答案（ResNet论文中明确说明优化困难并不是由于梯度消失而导致的），但其实也正是我希望得到的回答。

!!! question "梯度消失"

    什么是梯度消失？传统的激活函数在两端会进入梯度饱和区，从而导致梯度消失。但现代激活函数比如ReLU，他在输入为正时恒为1，那么应当就没有梯度消失问题才对啊？

从这个问题开始就在挖坑了。ReLU在输入为负时恒为0，因此对梯度消失的效果有限。

!!! success "梯度消失/爆炸"

    梯度消失/爆炸问题主要是通过BN和初始化来解决的。

## 归一化

!!! question "BatchNorm"

    什么是BatchNorm？BatchNorm如何计算并起到了什么样的效果？

!!! failure "ICS"

    BatchNorm通过将输入变换成一个均值为0，方差为1的分布，从而缓解了Internal Covariant Shift。

这个答案其实还是一个可接受的答案--Inception V2里正是这么解释的。然而后来的研究发现给BN的输出人为加一个随机的偏移，使其仍然存在ICS，BN的效果一样的好。

!!! success "利普希茨"

    BatchNorm通过使损失的landscape更加平滑从而降低了优化的难度。

这是目前对BatchNorm的解释之一，当然此外还有写文章分析了BatchNorm可以缓解Mean Shift，答出这些说明候选人的阅读范围十分广泛，算是一个加分项。

!!! question "归一化"

    你还了解其他的归一化方法吗？

这个属于一个开放式的扩展问题，在候选人展现出对BN的深刻理解之后一定会有。如上一道问题一样，我也不期望候选人能给出令人欣赏的回答。

!!! success "归一化"

    常用归一化包括BatchNorm、LayerNorm、InstanceNorm以及后两者的杂交GroupNorm。如果把数据看作一个$[N, C, HW]$的正方体的话，BN对NHW做归一化、LN对CHW做归一化，IN对HW做归一化，GN将C分成k组然后对kHW做归一化。

    BatchNorm在大批尺寸下表现一般，Sync BN可以一定程度上缓解这个问题，但由此引入的额外的通讯时间是令人非常难受的。由此，Google在BiT一文中使用GroupNorm并对卷积层应用Weight Standardization来取代BatchNorm。

## 初始化

!!! question "初始化"

    初始化是如何缓解梯度消失/梯度爆炸问题的呢？为什么有$Xavier$、$Kaiming$这样的初始化呢？

!!! success "初始化"

    我们通常希望网络的输出是一个均值为0方差为1的标准正态分布--与输入保持一致。

    对于一个只有卷积或者全连接层构成的线性的神经网络来说，常规做法是使用$1 / n_{in}$（$n_{in}$为输入连接数）来归一化权重使得其输出满足正态分布。然而，这个做法在引入了非线性变换的神经网络中效果有限，Xavier认为反向传播也很重要，因此将上式变为$2 / n_{in} + n_{out}$（$n_{out}$为输出连接数）。$Xavier$对于传统非线性激活函数效果还凑合，但对$ReLU$这样的激活函数就跪了。因此有了$Kaiming$初始化。

## 网络退化

对于一个深度为5层的网络，我们在其后添上45层的恒等连接构建成一个50层神经网络，这个神经网络应该和5层网络效果完全一致。也就是说，一个深度神经网络至少会跟浅度网络一样好。但是，实际训练中我们经常发现5层的网络比50层的效果要好。也就是说网络发生了退化。

现有理论认为随着网络深度的增加，隐藏层的有效维度逐渐降低、积矩阵的奇异值越来越集中0，这使得优化非常困难。

ResNet通过跳跃连接引入了一个恒等映射，来打破网络退化。原始论文认为，前向传播时，输入$X$通过残差连接可以直接被从浅层传播到深层；反向传播时，损失也可以直接从深层传播到浅层（从这个角度来说，ResNet确实会一定程度上缓解梯度消失问题：即使其中某一层的梯度很小也总会有损失通过残差连接传播上去）。之后则有研究对于残差块做了深入分析，他们认为残差网络展开后的路径具有一定的独立性和冗余性，因此贡献梯度的主要是相对较短的路径。

!!! abstract "后记"

    作为深度学习中最常见的架构，针对ResNet展开的研究几乎没有停过。本文所依赖的某些文章也许某天也会被证否。作为一个从业人员，坚持阅读论文跟进最新进展是十分重要的。想要拿到一个好的面试评价，这些也是必须的。
