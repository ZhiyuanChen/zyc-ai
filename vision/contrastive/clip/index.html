<!-- Google Tag Manager -->
<script>
  (function (w, d, s, l, i) {
    w[l] = w[l] || [];
    w[l].push({ "gtm.start": new Date().getTime(), event: "gtm.js" });
    var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s),
      dl = l != "dataLayer" ? "&l=" + l : "";
    j.async = true;
    j.src = "https://www.googletagmanager.com/gtm.js?id=" + i + dl;
    f.parentNode.insertBefore(j, f);
  })(window, document, "script", "dataLayer", "GTM-KV83JWN");
</script>
<!-- End Google Tag Manager -->
<!DOCTYPE html>

<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="The website of Zhiyuan Chen" name="description"/>
<meta content="Zhiyuan Chen" name="author"/>
<link href="https://zyc.ai/vision/contrastive/clip/" rel="canonical"/>
<link href="../../../transformer/early_convolutions/" rel="prev"/>
<link href="../SimCLR/" rel="next"/>
<link href="../../../feed_rss_created.xml" rel="alternate" title="RSS 订阅" type="application/rss+xml"/>
<link href="../../../feed_rss_updated.xml" rel="alternate" title="已更新内容的 RSS 订阅" type="application/rss+xml"/>
<link href="../../../assets/images/logo.ico" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.5.42" name="generator"/>
<title>CLIP - Zhiyuan Chen</title>
<link href="../../../assets/stylesheets/main.0253249f.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link href="../../../assets/css/fonts.css" rel="stylesheet"/>
<link href="../../../assets/css/extra.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-HJPHFP4Y4Y"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-HJPHFP4Y4Y",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-HJPHFP4Y4Y",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
<script>if("undefined"!=typeof __md_analytics){var consent=__md_get("__consent");consent&&consent.analytics&&__md_analytics()}</script>
</head>
<body data-md-color-accent="teal" data-md-color-primary="blue-grey" data-md-color-scheme="slate" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#clip-1">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="Zhiyuan Chen" class="md-header__button md-logo" data-md-component="logo" href="../../.." title="Zhiyuan Chen">
<img alt="logo" src="../../../assets/images/logo.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Zhiyuan Chen
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              CLIP
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="teal" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="blue-grey" data-md-color-scheme="slate" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="teal" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="blue-grey" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<a aria-label="分享" class="md-search__icon md-icon" data-clipboard="" data-clipboard-text="" data-md-component="search-share" href="javascript:void(0)" tabindex="-1" title="分享">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg>
</a>
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
<div class="md-search__suggest" data-md-component="search-suggest"></div>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/ZhiyuanChen" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    Zhiyuan Chen
  </div>
</a>
</div>
</nav>
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../..">
        
  
    
  
  Zhiyuan Chen

      </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../pytorch/nn/Module/">
          
  
    
  
  PyTorch

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../stat/">
          
  
    
  
  统计

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../optimisation/">
          
  
    
  
  优化

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../transformer/attention/">
          
  
    
  
  变换网络

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../../backbone/big_transfer/">
          
  
    
  
  视觉计算

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../language/word_processing/">
          
  
    
  
  自然语言计算

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../algorithm/">
          
  
    
  
  算法

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../container/k8s/">
          
  
    
  
  容器

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../sketch/chanfig/">
          
  
    
  
  随笔

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../about/zen/">
          
  
    
  
  关于

        </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Zhiyuan Chen" class="md-nav__button md-logo" data-md-component="logo" href="../../.." title="Zhiyuan Chen">
<img alt="logo" src="../../../assets/images/logo.png"/>
</a>
    Zhiyuan Chen
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/ZhiyuanChen" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
</div>
<div class="md-source__repository">
    Zhiyuan Chen
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../..">
<span class="md-ellipsis">
    Zhiyuan Chen
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../pytorch/nn/Module/">
<span class="md-ellipsis">
    PyTorch
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../stat/">
<span class="md-ellipsis">
    统计
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../optimisation/">
<span class="md-ellipsis">
    优化
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../transformer/attention/">
<span class="md-ellipsis">
    变换网络
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
<span class="md-ellipsis">
    视觉计算
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_6_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_6">
<span class="md-nav__icon md-icon"></span>
            视觉计算
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle md-toggle--indeterminate" id="__nav_6_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_6_1" id="__nav_6_1_label" tabindex="">
<span class="md-ellipsis">
    网络结构
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_6_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_6_1">
<span class="md-nav__icon md-icon"></span>
            网络结构
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../backbone/big_transfer/">
<span class="md-ellipsis">
    Big Transfer
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../transformer/visual_transformer/">
<span class="md-ellipsis">
    Visual Transformer
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../transformer/vision_transformer/">
<span class="md-ellipsis">
    Vision Transformer
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../transformer/early_convolutions/">
<span class="md-ellipsis">
    Early Convolutions
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_6_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_6_2" id="__nav_6_2_label" tabindex="">
<span class="md-ellipsis">
    对比学习
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_6_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_6_2">
<span class="md-nav__icon md-icon"></span>
            对比学习
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    CLIP
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    CLIP
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#_1">
<span class="md-ellipsis">
      动机
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_2">
<span class="md-ellipsis">
      原创
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_3">
<span class="md-ellipsis">
      方法
    </span>
</a>
<nav aria-label="方法" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_4">
<span class="md-ellipsis">
      模型
    </span>
</a>
<nav aria-label="模型" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_5">
<span class="md-ellipsis">
      视觉
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_6">
<span class="md-ellipsis">
      语言
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_7">
<span class="md-ellipsis">
      扩展
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_8">
<span class="md-ellipsis">
      数据
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_9">
<span class="md-ellipsis">
      训练
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_10">
<span class="md-ellipsis">
      实验
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../SimCLR/">
<span class="md-ellipsis">
    SimCLR
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../SimCLRv2/">
<span class="md-ellipsis">
    SimCLRv2
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../language/word_processing/">
<span class="md-ellipsis">
    自然语言计算
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../algorithm/">
<span class="md-ellipsis">
    算法
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../container/k8s/">
<span class="md-ellipsis">
    容器
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../sketch/chanfig/">
<span class="md-ellipsis">
    随笔
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../../about/zen/">
<span class="md-ellipsis">
    关于
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#_1">
<span class="md-ellipsis">
      动机
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_2">
<span class="md-ellipsis">
      原创
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_3">
<span class="md-ellipsis">
      方法
    </span>
</a>
<nav aria-label="方法" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_4">
<span class="md-ellipsis">
      模型
    </span>
</a>
<nav aria-label="模型" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#_5">
<span class="md-ellipsis">
      视觉
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_6">
<span class="md-ellipsis">
      语言
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_7">
<span class="md-ellipsis">
      扩展
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_8">
<span class="md-ellipsis">
      数据
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_9">
<span class="md-ellipsis">
      训练
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#_10">
<span class="md-ellipsis">
      实验
    </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<nav class="md-tags">
<span class="md-tag">Foundation Models</span>
<span class="md-tag">Vision Language</span>
</nav>
<h1 id="clip-1">CLIP <script type="math/tex; mode=display">1</script>
<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup><a class="headerlink" href="#clip-1" title="Permanent link">¶</a></h1>
<div class="admonition abstract">
<p class="admonition-title">摘要</p>
<p>本文提出一个新的预训练方式：预测图像的描述。
通过在4亿图像文本对（来自于互联网）上进行预训练，本文在超过30个数据集的多种下游任务上均达到了SOTA结果。
通过将自然语言与视觉概念进行关联，本文提出的方法在某些任务中无需任何数据微调即可达到SOTA结果，如ResNet-50 <script type="math/tex; mode=display">1</script>
<sup id="fnref4:2"><a class="footnote-ref" href="#fn:2">2</a></sup>在ImageNet <script type="math/tex; mode=display">1</script>
<sup id="fnref4:3"><a class="footnote-ref" href="#fn:3">3</a></sup>分类任务中。</p>
</div>
<h2 id="_1">动机<a class="headerlink" href="#_1" title="Permanent link">¶</a></h2>
<p>传统计算机视觉模型的主干网络通常都通过基于ImageNet <script type="math/tex; mode=display">1</script>
<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>图像分类任务的监督学习来进行训练。这使得他们极大的受限于有标注数据。
尽管以对比学习 <script type="math/tex; mode=display">2</script>
<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup><sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup>与重建学习 <script type="math/tex; mode=display">2</script>
<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup><sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup>为代表的自监督学习缓解了有标注数据不足的问题，但他们仍需要微调来应用在实际任务上。</p>
<h2 id="_2">原创<a class="headerlink" href="#_2" title="Permanent link">¶</a></h2>
<p><img alt="CLIP" src="main-diagrams.svg" title="CLIP"/></p>
<p>自Mori et al., (1999) <script type="math/tex; mode=display">1</script>
<sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup>以来，已经有数个方法来关联图像和其描述，但是他们的性能都不尽人意，Li et al., (2017) <script type="math/tex; mode=display">1</script>
<sup id="fnref:9"><a class="footnote-ref" href="#fn:9">9</a></sup>的方法在ImageNet <script type="math/tex; mode=display">1</script>
<sup id="fnref2:3"><a class="footnote-ref" href="#fn:3">3</a></sup>分类任务的零样本学习中只取得了<span class="arithmatex">\(11.5\%\)</span>的准确率，这与SOTA相去甚远。</p>
<p>此前已经有多个工作讨论在大规模数据集上通过弱监督进行训练 <script type="math/tex; mode=display">4</script>
<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">10</a></sup><sup id="fnref:11"><a class="footnote-ref" href="#fn:11">11</a></sup><sup id="fnref:12"><a class="footnote-ref" href="#fn:12">12</a></sup><sup id="fnref:13"><a class="footnote-ref" href="#fn:13">13</a></sup>，他们取得了出色的效果。
但是，他们都使用了一套精心设计的标签集，因而无法完全释放自然语言的潜能。
这限制了他们在零样本学习任务中的性能。</p>
<p>尽管MS-COCO <script type="math/tex; mode=display">1</script>
<sup id="fnref:14"><a class="footnote-ref" href="#fn:14">14</a></sup>和Visual Genome <script type="math/tex; mode=display">1</script>
<sup id="fnref:15"><a class="footnote-ref" href="#fn:15">15</a></sup>具有高质量且丰富的标注，他们只有约十万张图像因而不满足要求。
YFCC100M <script type="math/tex; mode=display">1</script>
<sup id="fnref:16"><a class="footnote-ref" href="#fn:16">16</a></sup>具有超过一亿张图像，但他们的元数据较为稀疏且质量良莠不齐。在通过判断标题和描述是否包含英文的自然语言进行筛选后，数据集仅剩约一千五百万张图像，与ImageNet <script type="math/tex; mode=display">1</script>
<sup id="fnref3:3"><a class="footnote-ref" href="#fn:3">3</a></sup>大小相当。</p>
<p>本文的核心贡献包括以下三点：</p>
<ol>
<li>本文提出了通过一个简化版的ConVIRT <script type="math/tex; mode=display">1</script>
<sup id="fnref2:17"><a class="footnote-ref" href="#fn:17">17</a></sup>结构来进行图像和其描述之间的对比学习。</li>
<li>本文构建了构建了一个有4亿张图片及对应的描述文本的大规模数据集上来训练所提出的模型。</li>
<li>本文构建了横跨两个数量级的八个模型来验证这个方法，结果表明迁移性能是一个与计算量有关的平滑的可预测方程 <script type="math/tex; mode=display">2</script>
<sup id="fnref:18"><a class="footnote-ref" href="#fn:18">18</a></sup><sup id="fnref:19"><a class="footnote-ref" href="#fn:19">19</a></sup>。</li>
</ol>
<h2 id="_3">方法<a class="headerlink" href="#_3" title="Permanent link">¶</a></h2>
<h3 id="_4">模型<a class="headerlink" href="#_4" title="Permanent link">¶</a></h3>
<p>本文将图像和文本分别经过一个独立的编码器来提取特征，然后通过一个线性映射层来将特征映射到同一空间。本文没有使用SimCLRv2 <script type="math/tex; mode=display">1</script>
<sup id="fnref:20"><a class="footnote-ref" href="#fn:20">20</a></sup>中额外的非线性映射层，因为他没有带来额外收益。本文猜想只有同一模态的对比学习才需要额外的非线性映射层。</p>
<h4 id="_5">视觉<a class="headerlink" href="#_5" title="Permanent link">¶</a></h4>
<p>本文探究了两种视觉编码器，ResNet <script type="math/tex; mode=display">1</script>
<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>和ViT <script type="math/tex; mode=display">1</script>
<sup id="fnref:21"><a class="footnote-ref" href="#fn:21">21</a></sup>。所有模型都没有使用任何预训练的权重，从头训练。</p>
<ol>
<li>ResNet <script type="math/tex; mode=display">1</script>
<sup id="fnref2:2"><a class="footnote-ref" href="#fn:2">2</a></sup>：本文使用了一个ResNet <script type="math/tex; mode=display">1</script>
<sup id="fnref3:2"><a class="footnote-ref" href="#fn:2">2</a></sup>的改进版ResNet-D <script type="math/tex; mode=display">1</script>
<sup id="fnref:22"><a class="footnote-ref" href="#fn:22">22</a></sup>模型。将池化层替换为Zhang (2019)的<code>antialised rect-2 blur</code>池化层。将最后的全局平均池化层替换为一个类似于自注意力机制的注意力池化层。</li>
<li>ViT <script type="math/tex; mode=display">1</script>
<sup id="fnref2:21"><a class="footnote-ref" href="#fn:21">21</a></sup>：本文几乎没有对ViT进行修改，除了在<code>stem</code>和<code>pos embed</code>之后加入了一个<code>LN</code>层。</li>
</ol>
<h4 id="_6">语言<a class="headerlink" href="#_6" title="Permanent link">¶</a></h4>
<p>本文使用了一个GPT-2 <script type="math/tex; mode=display">1</script>
<sup id="fnref:23"><a class="footnote-ref" href="#fn:23">23</a></sup>方式的Transformer <script type="math/tex; mode=display">1</script>
<sup id="fnref:24"><a class="footnote-ref" href="#fn:24">24</a></sup>来作为语言模型。
为了训练效率，序列长度被限制为76。
在句子的开始和结束有<code>[SOS]</code>和<code>[EOS]</code>令牌，最后一层的<code>[EOS]</code>令牌被用做文本的特征。</p>
<h4 id="_7">扩展<a class="headerlink" href="#_7" title="Permanent link">¶</a></h4>
<p>遵循EfficientNet <script type="math/tex; mode=display">1</script>
<sup id="fnref:25"><a class="footnote-ref" href="#fn:25">25</a></sup>，本文在扩展模型时同时扩展了视觉模型的深度宽度和分辨率；对于文本模型则只有宽度被扩展，本文发现CLIP对语言编码器的容量不敏感。</p>
<h3 id="_8">数据<a class="headerlink" href="#_8" title="Permanent link">¶</a></h3>
<p>本文从英文版维基百科 <script type="math/tex; mode=display">1</script>
<sup id="fnref:26"><a class="footnote-ref" href="#fn:26">26</a></sup>中筛选了所有出现至少100次的二元词组，最终构成了一个50万个词组的词典。
然后，根据词典中的查询词在互联网上爬取（图像，文本）二元组。
爬取得到的数据会按照查询词进行均衡，最终每个查询词约包括2万（图像，文本）对。
本文将这一数据集称为WIT (WebImageText)。
WIT的总单词量与GPT-2 <script type="math/tex; mode=display">1</script>
<sup id="fnref2:23"><a class="footnote-ref" href="#fn:23">23</a></sup>的WebText数据集的总单词量相当。</p>
<h3 id="_9">训练<a class="headerlink" href="#_9" title="Permanent link">¶</a></h3>
<p><img alt="效率" src="efficiency-ablation.svg" title="效率"/></p>
<p>最开始，和VirTex <script type="math/tex; mode=display">1</script>
<sup id="fnref:27"><a class="footnote-ref" href="#fn:27">27</a></sup>相似，本文联合训练一个图像卷积网络和文本变换网络来预测图像的描述。但是，这种方法速度很慢，难以高效的扩展。</p>
<p>本文认为这一部分是因为这个任务对每张图像预测其描述的确切原文。由于图像的描述具有巨大的多样性，这种训练非常困难。将文本预测目标从预测确切的原文更换为这些词的词袋向量，速度即可提升三倍。</p>
<p>此前研究发现对比学习比预测任务能得到更好的表示 <script type="math/tex; mode=display">1</script>
<sup id="fnref:28"><a class="footnote-ref" href="#fn:28">28</a></sup>，因此本文进一步将训练目标更换为类似于的对比学习，取得了额外的四倍提升。</p>
<p>本文的训练设置遵循了ConVIRT <script type="math/tex; mode=display">1</script>
<sup id="fnref:17"><a class="footnote-ref" href="#fn:17">17</a></sup>的实现，下图描述了损失函数的伪代码。</p>
<p><img alt="伪代码" src="pseudocode.svg" title="伪代码"/></p>
<h2 id="_10">实验<a class="headerlink" href="#_10" title="Permanent link">¶</a></h2>
<p>TODO</p>
<div class="footnote">
<hr/>
<ol>
<li id="fn:1">
<p>A. Radford <em>et al.</em>, “Learning transferable visual models from natural language supervision,” in <em>Proceedings of the 38<sup>th</sup> international conference on machine learning</em>, M. Meila and T. Zhang, Eds., in Proceedings of machine learning research, vol. 139. PMLR, 2021, pp. 8748–8763. Available: <a href="https://proceedings.mlr.press/v139/radford21a.html">https://proceedings.mlr.press/v139/radford21a.html</a> <a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</em>, 2016. <a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="#fnref3:2" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="#fnref4:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:3">
<p>J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li, “ImageNet: A large-scale hierarchical image database,” in <em>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</em>, 2009, pp. 248–255. <a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">↩</a><a class="footnote-backref" href="#fnref2:3" title="Jump back to footnote 3 in the text">↩</a><a class="footnote-backref" href="#fnref3:3" title="Jump back to footnote 3 in the text">↩</a><a class="footnote-backref" href="#fnref4:3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:4">
<p>K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual representation learning,” in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2020. <a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
<li id="fn:5">
<p>T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” in <em>Proceedings of the 37<sup>th</sup> international conference on machine learning</em>, H. D. III and A. Singh, Eds., in Proceedings of machine learning research, vol. 119. PMLR, 2020, pp. 1597–1607. Available: <a href="https://proceedings.mlr.press/v119/chen20j.html">https://proceedings.mlr.press/v119/chen20j.html</a> <a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">↩</a></p>
</li>
<li id="fn:6">
<p>K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2022, pp. 16000–16009. <a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">↩</a></p>
</li>
<li id="fn:7">
<p>H. Bao, L. Dong, S. Piao, and F. Wei, “BEiT: BERT pre-training of image transformers,” in <em>International conference on learning representations</em>, 2022. Available: <a href="https://openreview.net/forum?id=p-BhZSz59o4">https://openreview.net/forum?id=p-BhZSz59o4</a> <a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">↩</a></p>
</li>
<li id="fn:8">
<p>Y. Mori, H. Takahashi, and R. Oka, “Image-to-word transformation based on dividing and vector quantizing images with words,” in <em>First international workshop on multimedia intelligent storage and retrieval management</em>, Citeseer, 1999, pp. 1–9. <a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">↩</a></p>
</li>
<li id="fn:9">
<p>A. Li, A. Jabri, A. Joulin, and L. van der Maaten, “Learning visual n-grams from web data,” in <em>Proceedings of the IEEE international conference on computer vision (ICCV)</em>, 2017. <a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">↩</a></p>
</li>
<li id="fn:10">
<p>C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting unreasonable effectiveness of data in deep learning era,” in <em>Proceedings of the IEEE international conference on computer vision (ICCV)</em>, 2017. <a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">↩</a></p>
</li>
<li id="fn:11">
<p>X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision transformers,” in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2022, pp. 12104–12113. <a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text">↩</a></p>
</li>
<li id="fn:12">
<p>Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, “Self-training with noisy student improves ImageNet classification,” in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2020. <a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text">↩</a></p>
</li>
<li id="fn:13">
<p>D. Mahajan <em>et al.</em>, “Exploring the limits of weakly supervised pretraining,” in <em>Proceedings of the european conference on computer vision (ECCV)</em>, 2018. <a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text">↩</a></p>
</li>
<li id="fn:14">
<p>T.-Y. Lin <em>et al.</em>, “Microsoft COCO: Common objects in context,” in <em>Proceedings of the european conference on computer vision (ECCV)</em>, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds., Springer International Publishing, 2014, pp. 740–755. Available: <a href="https://www.microsoft.com/en-us/research/publication/microsoft-coco-common-objects-in-context/">https://www.microsoft.com/en-us/research/publication/microsoft-coco-common-objects-in-context/</a> <a class="footnote-backref" href="#fnref:14" title="Jump back to footnote 14 in the text">↩</a></p>
</li>
<li id="fn:15">
<p>R. Krishna <em>et al.</em>, “Visual genome: Connecting language and vision using crowdsourced dense image annotations,” <em>International Journal of Computer Vision</em>, vol. 123, no. 1, pp. 32–73, May 2017, doi: <a href="https://doi.org/10.1007/s11263-016-0981-7">10.1007/s11263-016-0981-7</a>. <a class="footnote-backref" href="#fnref:15" title="Jump back to footnote 15 in the text">↩</a></p>
</li>
<li id="fn:16">
<p>B. Thomee <em>et al.</em>, “YFCC100M: The new data in multimedia research,” <em>Commun. ACM</em>, vol. 59, no. 2, pp. 64–73, Jan. 2016, doi: <a href="https://doi.org/10.1145/2812802">10.1145/2812802</a>. <a class="footnote-backref" href="#fnref:16" title="Jump back to footnote 16 in the text">↩</a></p>
</li>
<li id="fn:17">
<p>Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, “Contrastive learning of medical visual representations from paired images and text,” <em>CoRR</em>, vol. abs/2010.00747, 2020, Available: <a href="https://arxiv.org/abs/2010.00747">https://arxiv.org/abs/2010.00747</a> <a class="footnote-backref" href="#fnref:17" title="Jump back to footnote 17 in the text">↩</a><a class="footnote-backref" href="#fnref2:17" title="Jump back to footnote 17 in the text">↩</a></p>
</li>
<li id="fn:18">
<p>J. Hestness <em>et al.</em>, “Deep learning scaling is predictable, empirically,” <em>CoRR</em>, vol. abs/1712.00409, 2017, Available: <a href="http://arxiv.org/abs/1712.00409">http://arxiv.org/abs/1712.00409</a> <a class="footnote-backref" href="#fnref:18" title="Jump back to footnote 18 in the text">↩</a></p>
</li>
<li id="fn:19">
<p>J. Kaplan <em>et al.</em>, “Scaling laws for neural language models,” <em>CoRR</em>, vol. abs/2001.08361, 2020, Available: <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a> <a class="footnote-backref" href="#fnref:19" title="Jump back to footnote 19 in the text">↩</a></p>
</li>
<li id="fn:20">
<p>T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big self-supervised models are strong semi-supervised learners,” in <em>Advances in neural information processing systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., Curran Associates, Inc., 2020, pp. 22243–22255. Available: <a href="https://proceedings.neurips.cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/fcbc95ccdd551da181207c0c1400c655-Paper.pdf</a> <a class="footnote-backref" href="#fnref:20" title="Jump back to footnote 20 in the text">↩</a></p>
</li>
<li id="fn:21">
<p>A. Dosovitskiy <em>et al.</em>, “An image is worth 16x16 words: Transformers for image recognition at scale,” in <em>International conference on learning representations</em>, 2021. Available: <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a> <a class="footnote-backref" href="#fnref:21" title="Jump back to footnote 21 in the text">↩</a><a class="footnote-backref" href="#fnref2:21" title="Jump back to footnote 21 in the text">↩</a></p>
</li>
<li id="fn:22">
<p>T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li, “Bag of tricks for image classification with convolutional neural networks,” in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2019. <a class="footnote-backref" href="#fnref:22" title="Jump back to footnote 22 in the text">↩</a></p>
</li>
<li id="fn:23">
<p>A. Radford <em>et al.</em>, “Language models are unsupervised multitask learners,” 2018. <a class="footnote-backref" href="#fnref:23" title="Jump back to footnote 23 in the text">↩</a><a class="footnote-backref" href="#fnref2:23" title="Jump back to footnote 23 in the text">↩</a></p>
</li>
<li id="fn:24">
<p>A. Vaswani <em>et al.</em>, “Attention is all you need,” in <em>Advances in neural information processing systems</em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., Curran Associates, Inc., 2017. Available: <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a> <a class="footnote-backref" href="#fnref:24" title="Jump back to footnote 24 in the text">↩</a></p>
</li>
<li id="fn:25">
<p>M. Tan and Q. Le, “EfficientNet: Rethinking model scaling for convolutional neural networks,” in <em>Proceedings of the 36<sup>th</sup> international conference on machine learning</em>, K. Chaudhuri and R. Salakhutdinov, Eds., in Proceedings of machine learning research, vol. 97. PMLR, 2019, pp. 6105–6114. Available: <a href="https://proceedings.mlr.press/v97/tan19a.html">https://proceedings.mlr.press/v97/tan19a.html</a> <a class="footnote-backref" href="#fnref:25" title="Jump back to footnote 25 in the text">↩</a></p>
</li>
<li id="fn:26">
<p>W. Foundation, “Wikimedia downloads.” <script type="math/tex; mode=display">Online</script>. Available: <a href="https://dumps.wikimedia.org">https://dumps.wikimedia.org</a> <a class="footnote-backref" href="#fnref:26" title="Jump back to footnote 26 in the text">↩</a></p>
</li>
<li id="fn:27">
<p>K. Desai and J. Johnson, “VirTex: Learning visual representations from textual annotations,” in <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, 2021, pp. 11162–11173. <a class="footnote-backref" href="#fnref:27" title="Jump back to footnote 27 in the text">↩</a></p>
</li>
<li id="fn:28">
<p>Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” in <em>Proceedings of the european conference on computer vision (ECCV)</em>, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds., Cham: Springer International Publishing, 2020, pp. 776–794. <a class="footnote-backref" href="#fnref:28" title="Jump back to footnote 28 in the text">↩</a></p>
</li>
</ol>
</div>
<!-- Giscus -->
<script async="" crossorigin="anonymous" data-category="General" data-category-id="DIC_kwDOGv-RWM4CTx_d" data-emit-metadata="1" data-input-position="top" data-lang="en" data-loading="lazy" data-mapping="pathname" data-reactions-enabled="1" data-repo="ZhiyuanChen/DanLing" data-repo-id="R_kgDOGv-RWA" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js"></script>
<!-- Reload on palette change -->
<script>
  var palette = __md_get("__palette");
  if (palette && typeof palette.color === "object")
    if (palette.color.scheme === "slate") {
      var giscus = document.querySelector("script[src*=giscus]");
      giscus.setAttribute("data-theme", "dark");
    }

  /* Register event handlers after documented loaded */
  document.addEventListener("DOMContentLoaded", function () {
    var ref = document.querySelector("[data-md-component=palette]");
    ref.addEventListener("change", function () {
      var palette = __md_get("__palette");
      if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate" ? "dark" : "light";

        /* Instruct Giscus to change theme */
        var frame = document.querySelector(".giscus-frame");
        frame.contentWindow.postMessage(
          { giscus: { setConfig: { theme } } },
          "https://giscus.app",
        );
      }
    });
  });
</script>
<a class="md-content__icon pdf-download-btn" download href="../../../pandoc/vision/contrastive/clip/clip.pdf" title="Download"><i class="fa fas fa-download"></i><small> clip.pdf </small></a></article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
</main>
<footer class="md-footer">
<nav aria-label="页脚" class="md-footer__inner md-grid">
<a aria-label="上一页: Early Convolutions" class="md-footer__link md-footer__link--prev" href="../../../transformer/early_convolutions/">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                上一页
              </span>
<div class="md-ellipsis">
                Early Convolutions
              </div>
</div>
</a>
<a aria-label="下一页: SimCLR" class="md-footer__link md-footer__link--next" href="../SimCLR/">
<div class="md-footer__title">
<span class="md-footer__direction">
                下一页
              </span>
<div class="md-ellipsis">
                SimCLR
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
      All rights reserved © Zhiyuan Chen 2017-2024
    </div>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/ZhiyuanChen" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</a>
<a class="md-social__link" href="https://gitlab.com/ZhiyuanChen" rel="noopener" target="_blank" title="gitlab.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="m503.5 204.6-.7-1.8-69.7-181.78c-1.4-3.57-3.9-6.59-7.2-8.64-2.4-1.55-5.1-2.515-8-2.81s-5.7.083-8.4 1.11c-2.7 1.02-5.1 2.66-7.1 4.78-1.9 2.12-3.3 4.67-4.1 7.44l-47 144H160.8l-47.1-144c-.8-2.77-2.2-5.31-4.1-7.43-2-2.12-4.4-3.75-7.1-4.77a18.1 18.1 0 0 0-8.38-1.113 18.4 18.4 0 0 0-8.04 2.793 18.1 18.1 0 0 0-7.16 8.64L9.267 202.8l-.724 1.8a129.57 129.57 0 0 0-3.52 82c7.747 26.9 24.047 50.7 46.447 67.6l.27.2.59.4 105.97 79.5 52.6 39.7 32 24.2c3.7 1.9 8.3 4.3 13 4.3s9.3-2.4 13-4.3l32-24.2 52.6-39.7 106.7-79.9.3-.3c22.4-16.9 38.7-40.6 45.6-67.5 8.6-27 7.4-55.8-2.6-82"></path></svg>
</a>
<a class="md-social__link" href="https://bitbucket.org/ZhiyuanChen" rel="noopener" target="_blank" title="bitbucket.org">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M22.2 32A16 16 0 0 0 6 47.8a26 26 0 0 0 .2 2.8l67.9 412.1a21.77 21.77 0 0 0 21.3 18.2h325.7a16 16 0 0 0 16-13.4L505 50.7a16 16 0 0 0-13.2-18.3 25 25 0 0 0-2.8-.2zm285.9 297.8h-104l-28.1-147h157.3z"></path></svg>
</a>
<a class="md-social__link" href="mailto:this@zyc.ai" rel="noopener" target="_blank" title="">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"></path></svg>
</a>
<a class="md-social__link" href="https://www.facebook.com/zyc.fb" rel="noopener" target="_blank" title="www.facebook.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256c0 120 82.7 220.8 194.2 248.5V334.2h-52.8V256h52.8v-33.7c0-87.1 39.4-127.5 125-127.5 16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287v175.9C413.8 494.8 512 386.9 512 256"></path></svg>
</a>
<a class="md-social__link" href="https://twitter.com/zyc_ai" rel="noopener" target="_blank" title="twitter.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"></path></svg>
</a>
<a class="md-social__link" href="https://instagram.com/zyc.ai" rel="noopener" target="_blank" title="instagram.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141m0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7m146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8m76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8M398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1"></path></svg>
</a>
<a class="md-social__link" href="https://www.linkedin.com/in/%E9%99%9F%E5%8E%9F-%E9%99%88-0b473aa9/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<div class="md-progress" data-md-component="progress" role="progressbar"></div>
<div class="md-consent" data-md-component="consent" hidden="" id="__consent">
<div class="md-consent__overlay"></div>
<aside class="md-consent__inner">
<form class="md-consent__form md-grid md-typeset" name="consent">
<h4>Cookie Consent</h4>
<p>We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. With your consent, you're helping us to make our documentation better. Please check out our <a href="https://zyc.ai/about/privacy">Privacy Policy</a> for more information.</p>
<input class="md-toggle" id="__settings" type="checkbox"/>
<div class="md-consent__settings">
<ul class="task-list">
<li class="task-list-item">
<label class="task-list-control">
<input checked="" name="analytics" type="checkbox"/>
<span class="task-list-indicator"></span>
          Google Analytics
        </label>
</li>
<li class="task-list-item">
<label class="task-list-control">
<input checked="" name="github" type="checkbox"/>
<span class="task-list-indicator"></span>
          GitHub
        </label>
</li>
</ul>
</div>
<div class="md-consent__controls">
<button class="md-button md-button--primary">同意</button>
<label class="md-button" for="__settings">管理设定</label>
</div>
</form>
</aside>
</div>
<script>var consent=__md_get("__consent");if(consent)for(var input of document.forms.consent.elements)input.name&&(input.checked=consent[input.name]||!1);else"file:"!==location.protocol&&setTimeout((function(){document.querySelector("[data-md-component=consent]").hidden=!1}),250);var form=document.forms.consent;for(var action of["submit","reset"])form.addEventListener(action,(function(e){if(e.preventDefault(),"reset"===e.type)for(var n of document.forms.consent.elements)n.name&&(n.checked=!1);__md_set("__consent",Object.fromEntries(Array.from(new FormData(form).keys()).map((function(e){return[e,!0]})))),location.hash="",location.reload()}))</script>
<script id="__config" type="application/json">{"base": "../../..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
<script src="../../../assets/javascripts/bundle.83f73b43.min.js"></script>
<script src="../../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="../../../javascripts/shortcuts.js"></script>
</body>
</html>